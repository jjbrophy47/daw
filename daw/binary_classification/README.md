DAW Trees
---
DAW (Data AWare) trees save certain statistics at each node, making it easier to perform certain types of static analysis than using popular RF implementations such as from Scikit-Learn. 

### Learning

DAW trees follow standard RF learning procedures (e.g. sampling a random subset of features at each split in each tree; however, there is no option for boostrapping) with two significant differences controlled by the following hyperparameters.

* `k`: Controls the number of thresholds sampled for each feature at each node. If predictive performance is too low, try setting `k` to a larger value.


* `topd`: Any node at depth < `topd` will be a random node, meaning a feature is sampled unifomly at random, and a threshold is generated by sampling a value uniformly at random in the range [`min`, `max`] where `min` and `max` are the minimum and maximum values for the sampled feature.

### Statistics

DAW trees save special statistics at each decision node.

* `slack`: Minimum number of examples needed to retrain the node when ADDED (binary classification only).

### DAW vs DaRE

DAW trees differ from DaRE trees in the following ways:

* DAW does NOT keep a copy of the training data for each single tree/forest.
* DAW does NOT support the removal or addition of training examples.